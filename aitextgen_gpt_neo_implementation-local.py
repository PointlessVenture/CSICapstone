# -*- coding: utf-8 -*-
"""aitextgen GPT-NEO Implementation

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1WPgw7I9al9ndt3v7vy7kFp4Rn3uYuVjG
"""

# Starter imports/setup

# !pip install -q aitextgen

import logging
logging.basicConfig(
        format="%(asctime)s — %(levelname)s — %(name)s — %(message)s",
        datefmt="%m/%d/%Y %H:%M:%S",
        level=logging.INFO
    )

from aitextgen import aitextgen
#from aitextgen.colab import mount_gdrive, copy_file_from_gdrive

# GPU Verification

# !nvidia-smi
"""
# ai = aitextgen(tf_gpt2="124M", to_gpu=True)
def run_setup():
    # Comment out the above line and uncomment the below line to use GPT Neo instead.
    ai = aitextgen(model="EleutherAI/gpt-neo-125M", to_gpu=True)

    mount_gdrive()

    file_name = "FullDataset.txt"

    # If caching is used, (UNUSED FOR THIS IMPLEMENTATION, FOR NOW!)
    #copy_file_from_gdrive(file_name)

    # Run the fine-tune

    ai.train(file_name,
             line_by_line=False,
             from_cache=False,
             num_steps=3000,
             generate_every=1000,
             save_every=1000,
             save_gdrive=True,
             learning_rate=1e-3,
             fp16=False,
             batch_size=1, 
             )

    # Load the trained model if one already exists

    from_folder = "/content/drive/MyDrive/Capstone Project/GOODONEATG_20220327_013515"

    for file in ["pytorch_model.bin", "config.json"]:
      if from_folder:
        copy_file_from_gdrive(file, from_folder)
      else:
        copy_file_from_gdrive(file)


    # Reload the model if just trained, makes it easier to use.

    # ai = aitextgen(model_folder="trained_model", to_gpu=True)

# ai.generate()
"""
def generation():
    # load the retrained model + metadata necessary to generate text.
    ai = aitextgen(model_folder="C:\\Users\\collin.westgate\\PycharmProjects\\ComplimentbotLocal\\GOODONEATG_20220327_013515", to_gpu=False)

    ai.generate(n=5,
            batch_size=5,
            prompt="[person, brown hair]: \n 1. ",
            max_length=35,
            temperature=0.7,
            top_p=0.9)
